{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ad4b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sudo apt update\n",
    "#sudo apt install -y python3 python3-venv python3-pip\n",
    "#python3 -V                    # sanity check\n",
    "#python3 -m venv .venv\n",
    "#source .venv/bin/activate\n",
    "#python -m pip install --upgrade pip\n",
    "#pip install torch transformers evaluate rouge-score pandas\n",
    "#python -m pip install --upgrade pip setuptools wheel\n",
    "#pip install \"bitsandbytes>=0.43.3\" accelerate\n",
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1591d24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/ikt464project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  \n",
    "\n",
    "import re, json, numpy as np, pandas as pd, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7739d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen3-30B-A3B-Thinking-2507\"\n",
    "CSV_PATH = \"data/MeDAL/pretrain_subset/test.csv\"\n",
    "HUMAN_CSV = \"data/MeDAL/pretrain_subset/human_summaries_for_rouge.csv\"\n",
    "OUT_DIR = \"outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2f650eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2640f13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer & model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 16/16 [02:04<00:00,  7.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer & model...\")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        offload_state_dict=True,\n",
    "        offload_folder=\"offload\",\n",
    "        max_memory={0: \"30GiB\", \"cpu\": \"64GiB\"},\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        offload_state_dict=True,\n",
    "        offload_folder=\"offload\",\n",
    "        max_memory={\"cpu\": \"64GiB\"},\n",
    "    )\n",
    "\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "336a3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(tok, system_msg, user_text):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": user_text},\n",
    "    ]\n",
    "    chat = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return messages, chat\n",
    "\n",
    "def strip_all_think_blocks(t: str) -> str:\n",
    "    # Remove any nested <think>...</think> sections robustly\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", t, flags=re.DOTALL | re.IGNORECASE).strip()\n",
    "\n",
    "def safe_trim_to_first_n_sentences(text, n=4, min_keep=3):\n",
    "    text = strip_all_think_blocks(text)\n",
    "    # Also remove any leading \"Hmm,\" style prefaces\n",
    "    text = re.sub(r\"^\\s*(?:Hmm[.,].*?\\n+)+\", \"\", text, flags=re.IGNORECASE | re.DOTALL).strip()\n",
    "    sents = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "    if len(sents) > n:\n",
    "        sents = sents[:n]\n",
    "    if len(sents) < min_keep and len(text) > 0:\n",
    "        return text\n",
    "    return \" \".join(sents).strip()\n",
    "\n",
    "def postprocess(decoded: str) -> str:\n",
    "    # Keep only the text after the LAST </think> (if any slipped through)\n",
    "    if \"</think>\" in decoded:\n",
    "        decoded = decoded.split(\"</think>\")[-1]\n",
    "    # If <summary>...</summary> present, extract the inner text\n",
    "    m = re.search(r\"<summary>(.*?)</summary>\", decoded, flags=re.DOTALL | re.IGNORECASE)\n",
    "    final = (m.group(1) if m else decoded).strip()\n",
    "    # Normalize empty/ellipsis\n",
    "    if final.strip() in {\"\", \"...\", \"…\"}:\n",
    "        return \"\"\n",
    "    return safe_trim_to_first_n_sentences(final, n=4, min_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7b76869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def get_embedding_device(model):\n",
    "    try:\n",
    "        return model.get_input_embeddings().weight.device\n",
    "    except Exception:\n",
    "        return next(model.parameters()).device\n",
    "\n",
    "EMBED_DEVICE = get_embedding_device(model)\n",
    "print(\"Embedding device:\", EMBED_DEVICE)\n",
    "\n",
    "class StopOnSubstrings(StoppingCriteria):\n",
    "    \"\"\"\n",
    "    Stop generation once any of the provided stop strings appears at the end (token-wise).\n",
    "    This is complemented by a decoded-text cut for robustness.\n",
    "    \"\"\"\n",
    "    def __init__(self, stop_strings, tokenizer):\n",
    "        super().__init__()\n",
    "        self.stop_ids = [tokenizer.encode(s, add_special_tokens=False) for s in stop_strings]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        last_tokens = input_ids[0].tolist()\n",
    "        for s_ids in self.stop_ids:\n",
    "            if len(last_tokens) >= len(s_ids) and last_tokens[-len(s_ids):] == s_ids:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "STOP_STRINGS = [\"</summary>\"]\n",
    "STOPPER = StoppingCriteriaList([StopOnSubstrings(STOP_STRINGS, tok)])\n",
    "\n",
    "def cut_after_last_summary_tag(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Hard-cut decoded text right after the first occurrence of </summary>.\n",
    "    This complements token-level stopping and ensures a clean end.\n",
    "    \"\"\"\n",
    "    end_tag = \"</summary>\"\n",
    "    pos = text.find(end_tag)\n",
    "    if pos != -1:\n",
    "        return text[:pos + len(end_tag)]\n",
    "    return text\n",
    "\n",
    "def get_model_ctx(model, default_ctx=262144):\n",
    "    \"\"\"\n",
    "    Try to read the model's max context length; fall back to a large safe default.\n",
    "    \"\"\"\n",
    "    ctx = getattr(getattr(model, \"config\", object()), \"max_position_embeddings\", None)\n",
    "    if isinstance(ctx, int) and ctx > 0:\n",
    "        return ctx\n",
    "    return default_ctx  # Qwen3 models advertise very long contexts\n",
    "\n",
    "def generate_summary(model, tok, chat, system_msg_for_rebuild,\n",
    "                     keep_tokens_for_answer=512, source_text=None,\n",
    "                     gen_temp=0.4, gen_top_p=0.9, greedy=False):\n",
    "    # Determine context and set a prompt budget\n",
    "    max_ctx = int(get_model_ctx(model))\n",
    "    reserve = keep_tokens_for_answer + 64\n",
    "    prompt_budget = max(256, max_ctx - reserve)\n",
    "\n",
    "    # Tokenize + move to embedding device\n",
    "    inputs = tok([chat], return_tensors=\"pt\", truncation=True, max_length=prompt_budget)\n",
    "    inputs = {k: v.to(EMBED_DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "    # Rebuild from head if still too long\n",
    "    if inputs[\"input_ids\"].shape[1] >= prompt_budget and source_text:\n",
    "        head_enc = tok(source_text, return_tensors=\"pt\", truncation=True, max_length=prompt_budget)\n",
    "        trimmed = tok.decode(head_enc[\"input_ids\"][0], skip_special_tokens=True)\n",
    "        _, chat2 = build_inputs(tok, system_msg_for_rebuild, trimmed)\n",
    "        inputs = tok([chat2], return_tensors=\"pt\", truncation=True, max_length=prompt_budget)\n",
    "        inputs = {k: v.to(EMBED_DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "    # Ensure pad token again (defensive)\n",
    "    if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=320,\n",
    "        eos_token_id=tok.eos_token_id,     # allow EOS to stop as well\n",
    "        stopping_criteria=STOPPER,         # primary hard stop at </summary>\n",
    "        no_repeat_ngram_size=3,\n",
    "    )\n",
    "    if greedy:\n",
    "        gen_kwargs.update(dict(do_sample=False))\n",
    "    else:\n",
    "        gen_kwargs.update(dict(do_sample=True, temperature=gen_temp, top_p=gen_top_p))\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "    # Decode only the newly generated tokens\n",
    "    new_tokens = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    decoded = tok.decode(new_tokens, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    # Hard-cut after </summary> in decoded space for robustness\n",
    "    decoded = cut_after_last_summary_tag(decoded)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b14524e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV sample. Characters: 1039\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    source_text = str(df.iloc[0][\"TEXT\"]).strip()\n",
    "    # capture ABSTRACT_ID if present for alignment with human references\n",
    "    abs_id = int(df.iloc[0][\"ABSTRACT_ID\"]) if \"ABSTRACT_ID\" in df.columns else None\n",
    "    print(\"Loaded CSV sample. Characters:\", len(source_text))\n",
    "except Exception as e:\n",
    "    print(\"CSV load warning:\", e)\n",
    "    source_text = (\n",
    "        \"Background: Hypertension is a common cardiovascular risk factor. \"\n",
    "        \"Methods: We conducted a randomized, controlled trial evaluating a new ACE inhibitor versus placebo \"\n",
    "        \"in 1,200 adults with stage 2 hypertension over 24 weeks. \"\n",
    "        \"Results: The treatment group showed a mean systolic BP reduction of 18 mmHg versus 6 mmHg with placebo; \"\n",
    "        \"adverse events were mild and comparable. \"\n",
    "        \"Conclusion: The ACE inhibitor significantly reduced blood pressure with acceptable safety.\"\n",
    "    )\n",
    "    abs_id = None\n",
    "    print(\"Using fallback abstract. Characters:\", len(source_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ec046d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MSG_TAGGED = (\n",
    "    \"Summarize the user's medical abstract in 3–4 sentences. \"\n",
    "    \"Be clear and factual. Keep key clinical details (condition, intervention, measurements, outcomes). \"\n",
    "    \"Return ONLY the summary wrapped exactly as:\\n<summary>...</summary>\\nNo preface, no analysis, no extra text.\"\n",
    ")\n",
    "\n",
    "SYSTEM_MSG_FALLBACK = (\n",
    "    \"Summarize the user's medical abstract in 3–4 sentences. \"\n",
    "    \"Be clear and factual. Keep key clinical details (condition, intervention, measurements, outcomes). \"\n",
    "    \"Output ONLY the 3–4 sentence summary—no preface, no analysis.\"\n",
    ")\n",
    "\n",
    "GEN_TEMP, GEN_TOPP = 0.4, 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "511e993b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary (attempt 1)...\n",
      "\n",
      "--- SUMMARY ---\n",
      "\n",
      "Looking at the abstract, it describes a sheep model of allergic airWAY disease. The researchers exposed sheep to Ascaris allergen until they developed chronic inflammation. They divided the sheep into three groups: Group A (control, saline only), Group B (non-reactive), and Group C (reactive with stable increased airway resistance). Key measurements included airway mechanics (RL, FRC), lung function tests (plethysomography, helium rebreathe), and BAL analyses (cell counts, protein levels).\n"
     ]
    }
   ],
   "source": [
    "# Attempt 1\n",
    "_, chat = build_inputs(tok, SYSTEM_MSG_TAGGED, source_text)\n",
    "print(\"Generating summary (attempt 1)...\")\n",
    "decoded = generate_summary(\n",
    "    model, tok, chat, system_msg_for_rebuild=SYSTEM_MSG_TAGGED,\n",
    "    keep_tokens_for_answer=512, source_text=source_text,\n",
    "    gen_temp=GEN_TEMP, gen_top_p=GEN_TOPP, greedy=False\n",
    ")\n",
    "summary = postprocess(decoded)\n",
    "\n",
    "# Fallback: deterministic pass\n",
    "if not summary:\n",
    "    print(\"Generating summary (attempt 2, deterministic fallback)...\")\n",
    "    _, chat_fb = build_inputs(tok, SYSTEM_MSG_FALLBACK, source_text)\n",
    "    decoded_fb = generate_summary(\n",
    "        model, tok, chat_fb, system_msg_for_rebuild=SYSTEM_MSG_FALLBACK,\n",
    "        keep_tokens_for_answer=512, source_text=source_text,\n",
    "        gen_temp=0.7, gen_top_p=0.95, greedy=True\n",
    "    )\n",
    "    summary = postprocess(decoded_fb)\n",
    "\n",
    "# Final guardrail: strip leading lone \"I ...\" lines\n",
    "if summary and summary[:1].lower() == \"i\":\n",
    "    summary = re.sub(r\"(^|\\n)I[^\\n]*\", \"\", summary).strip()\n",
    "\n",
    "if not summary:\n",
    "    preview = (decoded or \"\")[:400].replace(\"\\n\", \" \")\n",
    "    print(\"\\n[Debug] Model raw (first 400 chars):\", preview)\n",
    "    summary = \"Summary unavailable: the model did not produce a clean summary.\"\n",
    "\n",
    "print(\"\\n--- SUMMARY ---\\n\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b761524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved to outputs/summary_first_test.txt\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "out_path = \"outputs/summary_first_test.txt\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary + \"\\n\")\n",
    "print(f\"\\nSaved to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c99321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TF-IDF EVAL ---\n",
      "tfidf_cosine_similarity: 0.2578\n",
      "coverage@0.10: 1.0000\n",
      "redundancy_avg_pairwise: 0.0188\n",
      "top_keywords: ['sheep', 'group', 'airway', 'reactive', 'stable', 'sheep model', 'sheep groups', 'sheep ascaris', 'saline group', 'saline']\n",
      "notes: Higher similarity & coverage are good; lower redundancy is better.\n",
      "Saved TF-IDF metrics to outputs/tfidf_eval.json\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# TF-IDF evaluation \n",
    "# -----------------------------------------------------------------------------\n",
    "def split_sentences(text: str):\n",
    "    return [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n",
    "\n",
    "def _safe_max_df(n_docs: int, max_df):\n",
    "   \n",
    "    if isinstance(max_df, float):\n",
    "        if n_docs <= 10:\n",
    "            return 1.0\n",
    "        return max_df\n",
    "    return max_df  \n",
    "\n",
    "def tfidf_eval(source_text: str, summary: str,\n",
    "               ngram_range=(1,2), max_df=0.9, min_df=1,\n",
    "               coverage_threshold=0.10, use_stopwords=True):\n",
    "   \n",
    "    stop = 'english' if use_stopwords else None\n",
    "\n",
    "    # --- Global similarity (2 docs) ---\n",
    "    docs = [source_text, summary]\n",
    "    vec_global = TfidfVectorizer(lowercase=True, stop_words=stop,\n",
    "                                 ngram_range=ngram_range,\n",
    "                                 max_df=_safe_max_df(len(docs), max_df),\n",
    "                                 min_df=min_df)\n",
    "    X = vec_global.fit_transform(docs)\n",
    "    global_sim = float(cosine_similarity(X[0], X[1])[0, 0])\n",
    "\n",
    "    # --- Coverage (many src sentences + 1 summary) ---\n",
    "    src_sents = split_sentences(source_text)[:200] or [source_text]\n",
    "    cov_docs = src_sents + [summary]\n",
    "    vec_cov = TfidfVectorizer(lowercase=True, stop_words=stop,\n",
    "                              ngram_range=ngram_range,\n",
    "                              max_df=_safe_max_df(len(cov_docs), max_df),\n",
    "                              min_df=min_df)\n",
    "    X_cov = vec_cov.fit_transform(cov_docs)\n",
    "    S, q = X_cov[:-1], X_cov[-1]\n",
    "    sent_sims = cosine_similarity(S, q).ravel()\n",
    "    coverage = float((sent_sims >= coverage_threshold).mean())\n",
    "\n",
    "    # --- Redundancy (within-summary) ---\n",
    "    summ_sents = split_sentences(summary)\n",
    "    if len(summ_sents) >= 2:\n",
    "        vec_red = TfidfVectorizer(lowercase=True, stop_words=stop,\n",
    "                                  ngram_range=ngram_range,\n",
    "                                  max_df=1.0, min_df=1)  \n",
    "        X_red = vec_red.fit_transform(summ_sents)\n",
    "        C = cosine_similarity(X_red)\n",
    "        redundancy = float((C.sum() - np.trace(C)) / (C.shape[0]*C.shape[1] - C.shape[0]))\n",
    "    else:\n",
    "        redundancy = 0.0\n",
    "\n",
    "    # --- Top keywords (single doc: summary) ---\n",
    "    top_keywords = []\n",
    "    if len(re.findall(r\"\\w+\", summary)) >= 2:\n",
    "        vec_kw = TfidfVectorizer(lowercase=True, stop_words=stop,\n",
    "                                 ngram_range=ngram_range,\n",
    "                                 max_df=1.0, min_df=1)  \n",
    "        try:\n",
    "            X_kw = vec_kw.fit_transform([summary])\n",
    "            vocab = np.array(vec_kw.get_feature_names_out())\n",
    "            scores = X_kw.toarray()[0]\n",
    "            top_idx = scores.argsort()[::-1][:10]\n",
    "            top_keywords = [(vocab[i], float(scores[i])) for i in top_idx if scores[i] > 0]\n",
    "        except ValueError:\n",
    "            top_keywords = []\n",
    "\n",
    "    return {\n",
    "        \"tfidf_cosine_similarity\": global_sim,\n",
    "        f\"coverage@{coverage_threshold:.2f}\": coverage,\n",
    "        \"redundancy_avg_pairwise\": redundancy,\n",
    "        \"top_keywords\": top_keywords,\n",
    "        \"notes\": \"Higher similarity & coverage are good; lower redundancy is better.\"\n",
    "    }\n",
    "\n",
    "metrics = tfidf_eval(\n",
    "    source_text,\n",
    "    summary,\n",
    "    ngram_range=(1,2),\n",
    "    max_df=0.9,\n",
    "    min_df=1,\n",
    "    coverage_threshold=0.10,\n",
    "    use_stopwords=True  \n",
    ")\n",
    "\n",
    "print(\"\\n--- TF-IDF EVAL ---\")\n",
    "for k, v in metrics.items():\n",
    "    if k == \"top_keywords\":\n",
    "        print(f\"{k}: {[w for w,_ in v]}\")\n",
    "    else:\n",
    "        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "with open(\"outputs/tfidf_eval.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "print(\"Saved TF-IDF metrics to outputs/tfidf_eval.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a4b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ROUGE EVAL ---\n",
      "ROUGE1    F1=0.4714\n",
      "ROUGE2    F1=0.1014\n",
      "ROUGELSUM F1=0.2429\n",
      "Saved ROUGE results to outputs/rouge_per_sample.json and outputs/rouge_aggregate.json\n"
     ]
    }
   ],
   "source": [
    "# ============================ ROUGE evaluation ==============================\n",
    "def _find_ref_col(df_refs: pd.DataFrame) -> str:\n",
    "    for c in [\"HUMAN_SUMMARY\", \"SUMMARY\", \"reference\", \"REF\"]:\n",
    "        if c in df_refs.columns:\n",
    "            return c\n",
    "    raise ValueError(\n",
    "        \"Could not find a reference-summary column in HUMAN_CSV. \"\n",
    "        \"Expected one of: HUMAN_SUMMARY, SUMMARY, reference, REF.\"\n",
    "    )\n",
    "\n",
    "def _to_plain_text(t: str) -> str:\n",
    "    t = strip_all_think_blocks(str(t))\n",
    "    t = re.sub(r\"</?summary>\", \"\", t, flags=re.I)\n",
    "    return t.strip()\n",
    "\n",
    "def _load_human_reference(human_csv_path: str, abstract_id):\n",
    "    hdf = pd.read_csv(human_csv_path)\n",
    "    ref_col = _find_ref_col(hdf)\n",
    "\n",
    "    if abstract_id is not None and \"ABSTRACT_ID\" in hdf.columns:\n",
    "        hit = hdf.loc[hdf[\"ABSTRACT_ID\"] == abstract_id]\n",
    "        if not hit.empty:\n",
    "            refs = [str(r).strip() for r in hit[ref_col].dropna().tolist() if str(r).strip()]\n",
    "            if refs:\n",
    "                return refs\n",
    "\n",
    "    refs = [str(r).strip() for r in hdf[ref_col].dropna().tolist() if str(r).strip()]\n",
    "    if not refs:\n",
    "        raise ValueError(\"No non-empty human reference summaries found in HUMAN_CSV.\")\n",
    "    return refs\n",
    "\n",
    "print(\"\\n--- ROUGE EVAL ---\")\n",
    "try:\n",
    "    references = _load_human_reference(HUMAN_CSV, abs_id)\n",
    "    pred = _to_plain_text(summary)\n",
    "    refs_plain = [_to_plain_text(r) for r in references]\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeLsum\"], use_stemmer=True)\n",
    "\n",
    "    if len(refs_plain) > 1:\n",
    "        s = scorer.score_multi(refs_plain, pred)  \n",
    "    else:\n",
    "        s = scorer.score(refs_plain[0], pred)\n",
    "\n",
    "    per_sample_rows = []\n",
    "    def _row_from_score(tag, scr):\n",
    "        return {\n",
    "            \"metric\": tag,\n",
    "            \"precision\": float(scr.precision),\n",
    "            \"recall\": float(scr.recall),\n",
    "            \"f1\": float(scr.fmeasure),\n",
    "        }\n",
    "    for tag, scr in s.items():\n",
    "        per_sample_rows.append(_row_from_score(tag, scr))\n",
    "\n",
    "    agg = scoring.BootstrapAggregator()\n",
    "    agg.add_scores(s)\n",
    "    agg_res = agg.aggregate()\n",
    "\n",
    "    for tag in [\"rouge1\", \"rouge2\", \"rougeLsum\"]:\n",
    "        mid = agg_res[tag].mid\n",
    "        print(f\"{tag.upper():9s} F1={mid.fmeasure:.4f}\")\n",
    "\n",
    "    with open(os.path.join(OUT_DIR, \"rouge_per_sample.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(per_sample_rows, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    agg_out = {\n",
    "        k: {\n",
    "            \"precision\": float(v.mid.precision),\n",
    "            \"recall\": float(v.mid.recall),\n",
    "            \"f1\": float(v.mid.fmeasure),\n",
    "            \"low_f1\": float(v.low.fmeasure),\n",
    "            \"high_f1\": float(v.high.fmeasure),\n",
    "        }\n",
    "        for k, v in agg_res.items()\n",
    "    }\n",
    "    with open(os.path.join(OUT_DIR, \"rouge_aggregate.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(agg_out, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"Saved ROUGE results to outputs/rouge_per_sample.json and outputs/rouge_aggregate.json\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"ROUGE evaluation error:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
