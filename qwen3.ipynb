{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "66ad4b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sudo apt update\n",
    "#sudo apt install -y python3 python3-venv python3-pip\n",
    "#python3 -V                    # sanity check\n",
    "#python3 -m venv .venv\n",
    "#source .venv/bin/activate\n",
    "#python -m pip install --upgrade pip\n",
    "#pip install torch transformers evaluate rouge-score pandas\n",
    "#python -m pip install --upgrade pip setuptools wheel\n",
    "#pip install \"bitsandbytes>=0.43.3\" accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1591d24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os, re, pandas as pd, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Mitigate CUDA allocator fragmentation (set before CUDA allocations)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7739d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen3-30B-A3B-Thinking-2507\"\n",
    "CSV_PATH = \"data/MeDAL/pretrain_subset/test.csv\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a2f650eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2640f13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer & model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 16/16 [02:24<00:00,  9.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer & model...\")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # GPU path: 4-bit quant + automatic device placement & offload\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",                \n",
    "        torch_dtype=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        offload_state_dict=True,\n",
    "        offload_folder=\"offload\",\n",
    "        max_memory={0: \"30GiB\", \"cpu\": \"64GiB\"},  \n",
    "    )\n",
    "else:\n",
    "    # CPU fallback (very slow for a 30B model, but keeps code error-free)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        offload_state_dict=True,\n",
    "        offload_folder=\"offload\",\n",
    "        max_memory={\"cpu\": \"64GiB\"},\n",
    "    )\n",
    "\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "336a3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(tok, system_msg, user_text):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": user_text},\n",
    "    ]\n",
    "    chat = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return messages, chat\n",
    "\n",
    "\n",
    "def safe_trim_to_first_n_sentences(text, n=4, min_keep=3):\n",
    "    text = text.replace(\"<think>\", \"\").replace(\"</think>\", \"\").strip()\n",
    "    text = re.sub(r\"^\\s*(?:Hmm[.,].*?\\n+)+\", \"\", text, flags=re.IGNORECASE | re.DOTALL).strip()\n",
    "    sents = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "    if len(sents) > n:\n",
    "        sents = sents[:n]\n",
    "    if len(sents) < min_keep and len(text) > 0:\n",
    "        return text\n",
    "    return \" \".join(sents).strip()\n",
    "\n",
    "\n",
    "def postprocess(decoded: str) -> str:\n",
    "    # Keep only the text after the LAST </think>\n",
    "    if \"</think>\" in decoded:\n",
    "        decoded = decoded.split(\"</think>\")[-1]\n",
    "    # Extract <summary>...</summary> if present\n",
    "    m = re.search(r\"<summary>(.*?)</summary>\", decoded, flags=re.DOTALL | re.IGNORECASE)\n",
    "    final = (m.group(1) if m else decoded).strip()\n",
    "    # Normalize empty/ellipsis\n",
    "    if final.strip() in {\"\", \"...\", \"…\"}:\n",
    "        return \"\"\n",
    "    return safe_trim_to_first_n_sentences(final, n=4, min_keep=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f7b76869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "\n",
    "def get_embedding_device(model):\n",
    "    try:\n",
    "        return model.get_input_embeddings().weight.device\n",
    "    except Exception:\n",
    "        return next(model.parameters()).device\n",
    "\n",
    "EMBED_DEVICE = get_embedding_device(model)\n",
    "print(\"Embedding device:\", EMBED_DEVICE)\n",
    "\n",
    "class StopOnSubstrings(StoppingCriteria):\n",
    "    \"\"\"\n",
    "    Stop generation once any of the provided stop strings appears at the end.\n",
    "    Compares on token space for reliability.\n",
    "    \"\"\"\n",
    "    def __init__(self, stop_strings, tokenizer):\n",
    "        super().__init__()\n",
    "        self.stop_ids = [tokenizer.encode(s, add_special_tokens=False) for s in stop_strings]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for s_ids in self.stop_ids:\n",
    "            if len(input_ids[0]) >= len(s_ids) and input_ids[0].tolist()[-len(s_ids):] == s_ids:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def make_bad_words_ids(tokenizer, bad_list):\n",
    "    ids = []\n",
    "    for w in bad_list:\n",
    "        wid = tokenizer.encode(w, add_special_tokens=False)\n",
    "        if wid:\n",
    "            ids.append(wid)\n",
    "    return ids\n",
    "\n",
    "STOP_STRINGS = [\"</summary>\"]\n",
    "BAD_WORDS = [\"I \", \"I'\", \"I'm\", \"I’m\", \"I need\", \"I will\", \"I’ll\", \"I think\", \"user\", \"preface\", \"analysis\"]\n",
    "\n",
    "STOPPER = StoppingCriteriaList([StopOnSubstrings(STOP_STRINGS, tok)])\n",
    "BAD_WORDS_IDS = make_bad_words_ids(tok, BAD_WORDS)\n",
    "\n",
    "def generate_summary(model, tok, chat, system_msg_for_rebuild,\n",
    "                     keep_tokens_for_answer=512, source_text=None,\n",
    "                     gen_temp=0.4, gen_top_p=0.9):\n",
    "    # Budget prompt\n",
    "    max_ctx = int(getattr(getattr(model, \"config\", object()), \"max_position_embeddings\", 32768))\n",
    "    reserve = keep_tokens_for_answer + 64\n",
    "    prompt_budget = max(256, max_ctx - reserve)\n",
    "\n",
    "    # Tokenize + move to embedding device\n",
    "    inputs = tok([chat], return_tensors=\"pt\", truncation=True, max_length=prompt_budget)\n",
    "    inputs = {k: v.to(EMBED_DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "    # Rebuild from head if still too long\n",
    "    if inputs[\"input_ids\"].shape[1] >= prompt_budget and source_text:\n",
    "        head_enc = tok(source_text, return_tensors=\"pt\", truncation=True, max_length=prompt_budget)\n",
    "        trimmed = tok.decode(head_enc[\"input_ids\"][0], skip_special_tokens=True)\n",
    "        _, chat2 = build_inputs(tok, system_msg_for_rebuild, trimmed)\n",
    "        inputs = tok([chat2], return_tensors=\"pt\", truncation=True, max_length=prompt_budget)\n",
    "        inputs = {k: v.to(EMBED_DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "    # Ensure pad token\n",
    "    if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=320,\n",
    "            do_sample=True,\n",
    "            temperature=gen_temp,\n",
    "            top_p=gen_top_p,\n",
    "            stopping_criteria=STOPPER,      # <- stop exactly at </summary>\n",
    "            bad_words_ids=BAD_WORDS_IDS,    # <- lightly discourage meta phrases\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "\n",
    "    # Decode only the newly generated tokens\n",
    "    new_tokens = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    decoded = tok.decode(new_tokens, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b14524e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV sample. Characters: 1039\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    source_text = str(df.iloc[0][\"TEXT\"]).strip()\n",
    "    if not source_text:\n",
    "        raise ValueError(\"Empty TEXT cell in CSV.\")\n",
    "    print(\"Loaded CSV sample. Characters:\", len(source_text))\n",
    "except Exception as e:\n",
    "    print(\"CSV load warning:\", e)\n",
    "    # Fallback text so the notebook runs without errors if CSV is missing\n",
    "    source_text = (\n",
    "        \"Background: Hypertension is a common cardiovascular risk factor. \"\n",
    "        \"Methods: We conducted a randomized, controlled trial evaluating a new ACE inhibitor versus placebo \"\n",
    "        \"in 1,200 adults with stage 2 hypertension over 24 weeks. \"\n",
    "        \"Results: The treatment group showed a mean systolic BP reduction of 18 mmHg versus 6 mmHg with placebo; \"\n",
    "        \"adverse events were mild and comparable. \"\n",
    "        \"Conclusion: The ACE inhibitor significantly reduced blood pressure with acceptable safety.\"\n",
    "    )\n",
    "    print(\"Using fallback abstract. Characters:\", len(source_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec046d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MSG_TAGGED = (\n",
    "    \"Summarize the user's medical abstract in 3–4 sentences. \"\n",
    "    \"Be clear and factual. Keep key clinical details (condition, intervention, measurements, outcomes). \"\n",
    "    \"Return ONLY the summary wrapped exactly as:\\n<summary>...</summary>\\nNo preface, no analysis, no extra text.\"\n",
    ")\n",
    "\n",
    "SYSTEM_MSG_FALLBACK = (\n",
    "    \"Summarize the user's medical abstract in 3–4 sentences. \"\n",
    "    \"Be clear and factual. Keep key clinical details (condition, intervention, measurements, outcomes). \"\n",
    "    \"Output ONLY the 3–4 sentence summary—no preface, no analysis.\"\n",
    ")\n",
    "\n",
    "GEN_TEMP, GEN_TOPP = 0.4, 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "511e993b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary (attempt 1)...\n",
      "\n",
      "--- SUMMARY ---\n",
      "\n",
      "Looking at this abstract, it describes a sheep model study where they exposed sheep to Ascaris allergen. The key points I need to extract: they created a chronic allergic model by repeated tracheally instilling Ascarist antigen until reactive sheep (Group C) showed stable 3x increase in airway resistance (RL) compared to controls. They compared Group C (reactive, n=6) to Group B (non-reactive, 3 sheep) and Group A (saline control, 8 sheep). Measurements included RL, FRC via two techniques, lung mechanics, and BAL cell counts.\n"
     ]
    }
   ],
   "source": [
    "# Attempt 1: tagged prompt that must end with </summary>\n",
    "_, chat = build_inputs(tok, SYSTEM_MSG_TAGGED, source_text)\n",
    "print(\"Generating summary (attempt 1)...\")\n",
    "decoded = generate_summary(\n",
    "    model, tok, chat, system_msg_for_rebuild=SYSTEM_MSG_TAGGED,\n",
    "    keep_tokens_for_answer=512, source_text=source_text,\n",
    "    gen_temp=GEN_TEMP, gen_top_p=GEN_TOPP\n",
    ")\n",
    "summary = postprocess(decoded)\n",
    "\n",
    "# Fallback if empty\n",
    "if not summary:\n",
    "    print(\"Generating summary (attempt 2, fallback)...\")\n",
    "    _, chat_fb = build_inputs(tok, SYSTEM_MSG_FALLBACK, source_text)\n",
    "    decoded_fb = generate_summary(\n",
    "        model, tok, chat_fb, system_msg_for_rebuild=SYSTEM_MSG_FALLBACK,\n",
    "        keep_tokens_for_answer=512, source_text=source_text,\n",
    "        gen_temp=0.7, gen_top_p=0.95\n",
    "    )\n",
    "    summary = postprocess(decoded_fb)\n",
    "    \n",
    "\n",
    "# Final guardrail: if the model STILL tries meta commentary, drop lines with “I …”\n",
    "if summary and summary[:1].lower() in {\"i\"}:\n",
    "    summary = re.sub(r\"(^|\\n)I[^\\n]*\", \"\", summary).strip()\n",
    "\n",
    "if not summary:\n",
    "    preview = (decoded or \"\")[:400].replace(\"\\n\", \" \")\n",
    "    print(\"\\n[Debug] Model raw (first 400 chars):\", preview)\n",
    "    summary = \"Summary unavailable: the model did not produce a clean summary.\"\n",
    "\n",
    "print(\"\\n--- SUMMARY ---\\n\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5b761524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved to outputs/summary_first_test.txt\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "out_path = \"outputs/summary_first_test.txt\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary + \"\\n\")\n",
    "print(f\"\\nSaved to {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
